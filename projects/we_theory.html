<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>David Alvarez-Melis | Towards a Theory of Word Embeddings</title>
  <meta name="description" content="Homepage of David Alvarez-Melis">
  <meta name="author" content="David Alvarez-Melis">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academic-Icons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>


  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

  <!-- Google Analytics -->
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7YM94T4RDX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7YM94T4RDX');
</script>

  

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <a href="/"><img title="Photo by: Eliza Grinell" class="u-max-full-width" src='/assets/profile-pics/Profile_pic_2021_Sq.jpg'></a>
        </div>
        <div class="nine columns main-description">
            <h1>David Alvarez-Melis</h1>
            <p>(he/him/his)</p>
            <p>Senior Researcher, Microsoft Research New England</p>
            <p>1 Memorial Drive, Cambridge MA</p>
            <p>firstsurname.secondsurname@microsoft.com</p>
            <p>
              <span onclick="window.open('https://twitter.com/elmelis')" style="cursor: pointer">
                <i class="fa fa-twitter" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://www.linkedin.com/in/davidalvarezmelis')" style="cursor: pointer">
                <i class="fa fa-linkedin-square" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://github.com/dmelis')" style="cursor: pointer">
                <i class="fa fa-github" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://scholar.google.com/citations?user=XsxZrYYAAAAJ&hl=en')" style="cursor: pointer">
                <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://www.semanticscholar.org/author/David-Alvarez-Melis/1390096054')" style="cursor: pointer">
                <i class="ai ai-semantic-scholar ai-lg" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://dblp.uni-trier.de/pers/hd/a/Alvarez=Melis:David')" style="cursor: pointer">
                <i class="ai ai-dblp ai-lg" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://orcid.org/0000-0002-9591-8986')" style="cursor: pointer">
                <i class="ai ai-orcid ai-lg" aria-hidden="true"></i>
              </span>
            </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=/index.html#about>About</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#bio>Bio</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#news>News</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications>Publications</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#projects>Projects</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#teaching>Teaching</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#resume>Vita</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#misc>Misc</a></li>
          <!-- <li class="navbar-item"><a class="navbar-link" href=/index.html#meta>Meta</a></li> -->
        </ul>
      </div>
    </nav>

    <div class="docs-section">

  
    <div class="title-subtitle">
      <h3>Towards a Theory of Word Embeddings</h3>
      <h5>A theoretical framework to understand the semantic properties of word embeddings.</h5>
    </div>
  

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>TL;DR:</strong> Word embeddings have intriguing semantic properties (think <em>King</em> - <em>Man</em> + <em>Woman</em> = <em>Queen</em>). How do we make sense of them? Cognitive Science and Metric Recovery yield a possible answer.</p>

<h5 id="abstract">Abstract</h5>

<p>Continuous word representations have been remarkably useful across NLP tasks but remain poorly understood. We ground word embeddings in semantic spaces studied in the cognitive-psychometric literature, taking these spaces as the primary objects to recover. To this end, we relate log co-occurrences of words in large corpora to semantic similarity assessments and show that co-occurrences are indeed consistent with an Euclidean semantic space hypothesis. Framing word embedding as metric recovery of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. Furthermore, we propose a simple, principled, direct metric recovery algorithm that performs on par with the state-of- the-art word embedding and manifold learning methods. Finally, we complement recent fo- cus on analogies by constructing two new inductive reasoning datasets—series completion and classification—and demonstrate that word embeddings can be used to solve them as well.</p>

<figure class="image">
  <img src="/assets/projects/we_theory/Sternberg.png" alt="The relation between semantics and word co-occurrence representations predates neural approaches by a few decades. This diagram depicts inductive reasoning in semantic spaces as proposed by Sternberg and Gardner (1983). A, B, C are
given, I is the ideal point and D are the choices." style="width: 100%" />
  <figcaption><b>The relation between semantics and word co-occurrence representations predates neural approaches by a few decades. This diagram depicts inductive reasoning in semantic spaces as proposed by Sternberg and Gardner (1983). A, B, C are
given, I is the ideal point and D are the choices.</b></figcaption>
</figure>

<figure class="image">
  <img src="/assets/projects/we_theory/Viz_Embeddings.png" alt="Our theory says that word embedding algorithms can be understood as manifold learning methods. Empirical results confirm this. Here, we show dimensionality reduction using both (word embedding and manifold learning) types of methods. Performance is quantified by percentage of 5-nearest neighbors sharing the same digit label. The resulting embeddings demonstrate that metric regression is highly effective at this task, outperforming metric SNE and beaten only by t-SNE (91% cluster purity), which is a visualization method specifically designed to preserve cluster separation. All word embedding methods including SVD (68%) embed the MNIST digits remarkably well and outperform classic manifold learning baselines of PCA (48%) and Isomap (49%)." style="width: 100%" />
  <figcaption><b>Our theory says that word embedding algorithms can be understood as manifold learning methods. Empirical results confirm this. Here, we show dimensionality reduction using both (word embedding and manifold learning) types of methods. Performance is quantified by percentage of 5-nearest neighbors sharing the same digit label. The resulting embeddings demonstrate that metric regression is highly effective at this task, outperforming metric SNE and beaten only by t-SNE (91% cluster purity), which is a visualization method specifically designed to preserve cluster separation. All word embedding methods including SVD (68%) embed the MNIST digits remarkably well and outperform classic manifold learning baselines of PCA (48%) and Isomap (49%).</b></figcaption>
</figure>

<!-- <figure class="image">
  <img src="/assets/projects/ot_we_alignment/GW_couplings.pdf" alt="." style="width: 100%"/>
  <figcaption><b>.</b></figcaption>
</figure>
 -->

<p><strong>Relevant Publications:</strong></p>
<ol>
  <li>Hashimoto, Alvarez-Melis and Jaakkola. “Word, graph and manifold embedding from Markov processes”, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning.</li>
  <li>Hashimoto, Alvarez-Melis and Jaakkola. “Word Embeddings as Metric Recovery in Semantic Spaces”, TACL’16.</li>
</ol>


</div>


    <div class="footer">
      <div class="row">
        <div class="four columns">
          David Alvarez-Melis
        </div>
        <div class="four columns">  &nbsp;&nbsp; </div>
        <!-- <div class="four columns">
          firstsurname.secondsurname@microsoft.com
        </div> -->
        <div class="four columns">
          <span onclick="window.open('https://twitter.com/elmelis')" style="cursor: pointer">
            <i class="fa fa-twitter" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://www.linkedin.com/in/davidalvarezmelis')" style="cursor: pointer">
            <i class="fa fa-linkedin-square" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://github.com/dmelis')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
